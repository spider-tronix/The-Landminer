{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_run.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNWluzuEsmURCat5Oy+baaI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-ZoyDQNskrCW","executionInfo":{"status":"ok","timestamp":1609489236544,"user_tz":-330,"elapsed":15209,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}},"outputId":"211ec31c-e5b4-4d3b-eaff-b90acfe2c2c7"},"source":["!pip install pyyaml==5.1\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pyyaml==5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n","\r\u001b[K     |█▏                              | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 30.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 22.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245kB 13.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256kB 13.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 13.2MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyyaml\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.1-cp36-cp36m-linux_x86_64.whl size=44075 sha256=7416c8f7ac19e16910a1ad2e88495cfe4074287c18a217fb452fabbdb0b00cd9\n","  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n","Successfully built pyyaml\n","Installing collected packages: pyyaml\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-5.1\n","Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n","Collecting detectron2\n","\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/detectron2-0.3%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.8MB)\n","\u001b[K     |████████████████████████████████| 6.8MB 750kB/s \n","\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.0.2)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.41.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.2)\n","Collecting Pillow>=7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 21.9MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0)\n","Collecting fvcore>=0.1.2\n","  Downloading https://files.pythonhosted.org/packages/8a/c8/46fccfaf9348e052e00b36765dee913dcd77a480bf3a9d4b4922602774ec/fvcore-0.1.2.post20201218.tar.gz\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.4.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7)\n","Collecting yacs>=0.1.6\n","  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.21)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.2->detectron2) (51.0.0)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot->detectron2) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.3.1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.19.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.8.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore>=0.1.2->detectron2) (5.1)\n","Collecting iopath>=0.1.2\n","  Downloading https://files.pythonhosted.org/packages/7a/9a/87a281c8cfc0ad1fceb228a4f854d02f19b2c2395476dd573327709b52ae/iopath-0.1.2.tar.gz\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.36.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.17.2)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.3.3)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.12.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.7.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.15.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.10.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.4.2)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.6)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (3.3.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.2.post20201218-cp36-none-any.whl size=40701 sha256=fe6c91a3cda235219a99f37e9104b3863beda3bb5b5647f90a9151fd3c7fcc1e\n","  Stored in directory: /root/.cache/pip/wheels/b5/83/3f/e74a72d264351b5ada512da845a5c6abb81a34f6333c3a54ce\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.2-cp36-none-any.whl size=10508 sha256=4a652d246c4a4a83f1677bef5e7c1558c9a9f3650e5c65816bec6c99acbf6bf8\n","  Stored in directory: /root/.cache/pip/wheels/9e/01/e4/1b68f5a2a6b9450ea4246d91840a77e1169f7d4722d76bbc47\n","Successfully built fvcore iopath\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: Pillow, yacs, portalocker, iopath, fvcore, detectron2\n","  Found existing installation: Pillow 7.0.0\n","    Uninstalling Pillow-7.0.0:\n","      Successfully uninstalled Pillow-7.0.0\n","Successfully installed Pillow-8.0.1 detectron2-0.3+cu101 fvcore-0.1.2.post20201218 iopath-0.1.2 portalocker-2.0.0 yacs-0.1.8\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0RO_TqrKT6d","executionInfo":{"status":"ok","timestamp":1609489300057,"user_tz":-330,"elapsed":32461,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}},"outputId":"89f726c9-5df7-43f6-e24c-a3d90ff31e24"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmhb-Ie-m8N6","executionInfo":{"status":"ok","timestamp":1609489313561,"user_tz":-330,"elapsed":5048,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}},"outputId":"833acdd7-6b78-4cef-9cde-f951a2f536c4"},"source":["import torch, torchvision\n","import numpy as np\n","import cv2\n","import json\n","import io\n","from detectron2.data.datasets import register_coco_instances\n","import detectron2\n","\n","\n","import os, random\n","\n","from detectron2.utils.logger import setup_logger\n","from detectron2.engine import DefaultTrainer\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","from detectron2.utils.visualizer import ColorMode\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","setup_logger()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["** fvcore version of PathManager will be deprecated soon. **\n","** Please migrate to the version in iopath repo. **\n","https://github.com/facebookresearch/iopath \n","\n","** fvcore version of PathManager will be deprecated soon. **\n","** Please migrate to the version in iopath repo. **\n","https://github.com/facebookresearch/iopath \n","\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<Logger detectron2 (DEBUG)>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"bcQFkRAQnIlf","executionInfo":{"status":"ok","timestamp":1609489316916,"user_tz":-330,"elapsed":916,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}}},"source":["dir=r\"/content/drive/MyDrive/wheat-coco/archive (1)/coco/\"\n","register_coco_instances(\"customDataset_train\", {}, dir+r\"annotations/instances_train2017.json\", dir+r\"images/train2017\")\n","register_coco_instances(\"customDataset_test\", {}, dir+r\"annotations/instances_val2017.json\", dir+r\"images/val2017\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItTYMw8dLylO","executionInfo":{"status":"ok","timestamp":1609489332028,"user_tz":-330,"elapsed":890,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}}},"source":["cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"customDataset_train\",)\n","cfg.DATASETS.TEST = (\"customDataset_test\", )\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025\n","cfg.SOLVER.MAX_ITER = 500\n","#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n","#cfg.MODEL.DEVICE=\"cpu\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnyMEKnFMDDp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609489798993,"user_tz":-330,"elapsed":461703,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}},"outputId":"6047c820-d450-46aa-97e3-a4df8d9fd32c"},"source":["os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()\n","\n","evaluator = COCOEvaluator(\"customDataset_test\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n","test_loader = build_detection_test_loader(cfg, \"customDataset_test\")\n","print(inference_on_dataset(trainer.model, test_loader, evaluator))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\u001b[32m[01/01 08:22:28 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[32m[01/01 08:22:31 d2.data.datasets.coco]: \u001b[0mLoading /content/drive/MyDrive/wheat-coco/archive (1)/coco/annotations/instances_train2017.json takes 2.17 seconds.\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/01 08:22:31 d2.data.datasets.coco]: \u001b[0m/content/drive/MyDrive/wheat-coco/archive (1)/coco/annotations/instances_train2017.json contains 130568 annotations, but only 40639 of them match to images in the file.\n","\u001b[32m[01/01 08:22:31 d2.data.datasets.coco]: \u001b[0mLoaded 1000 images in COCO format from /content/drive/MyDrive/wheat-coco/archive (1)/coco/annotations/instances_train2017.json\n","\u001b[32m[01/01 08:22:31 d2.data.build]: \u001b[0mRemoved 43 images with no usable annotations. 957 images left.\n","\u001b[32m[01/01 08:22:31 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|   wheat    | 40639        |\n","|            |              |\u001b[0m\n","\u001b[32m[01/01 08:22:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[01/01 08:22:31 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[01/01 08:22:31 d2.data.common]: \u001b[0mSerializing 957 elements to byte tensors and concatenating them all ...\n","\u001b[32m[01/01 08:22:31 d2.data.common]: \u001b[0mSerialized dataset takes 4.48 MiB\n"],"name":"stdout"},{"output_type":"stream","text":["model_final_f10217.pkl: 178MB [00:07, 24.2MB/s]                           \n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[01/01 08:22:44 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/detectron2/structures/masks.py:345: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n","/usr/local/lib/python3.6/dist-packages/detectron2/structures/masks.py:345: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n","/usr/local/lib/python3.6/dist-packages/detectron2/structures/masks.py:345: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n","/usr/local/lib/python3.6/dist-packages/detectron2/structures/masks.py:345: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n","/usr/local/lib/python3.6/dist-packages/detectron2/modeling/roi_heads/fast_rcnn.py:217: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  num_fg = fg_inds.nonzero().numel()\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[01/01 08:23:07 d2.utils.events]: \u001b[0m eta: 0:03:43  iter: 19  total_loss: 5.027  loss_cls: 0.8392  loss_box_reg: 0.3452  loss_mask: 0.695  loss_rpn_cls: 2.873  loss_rpn_loc: 0.2446  time: 0.4555  data_time: 0.7113  lr: 4.9953e-06  max_mem: 2486M\n","\u001b[32m[01/01 08:23:16 d2.utils.events]: \u001b[0m eta: 0:03:33  iter: 39  total_loss: 3.664  loss_cls: 0.7243  loss_box_reg: 0.439  loss_mask: 0.6878  loss_rpn_cls: 1.649  loss_rpn_loc: 0.2309  time: 0.4560  data_time: 0.0051  lr: 9.9902e-06  max_mem: 2486M\n","\u001b[32m[01/01 08:23:26 d2.utils.events]: \u001b[0m eta: 0:03:27  iter: 59  total_loss: 2.821  loss_cls: 0.6406  loss_box_reg: 0.6076  loss_mask: 0.6741  loss_rpn_cls: 0.7041  loss_rpn_loc: 0.2163  time: 0.4673  data_time: 0.0056  lr: 1.4985e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:23:36 d2.utils.events]: \u001b[0m eta: 0:03:21  iter: 79  total_loss: 2.529  loss_cls: 0.5951  loss_box_reg: 0.6758  loss_mask: 0.6544  loss_rpn_cls: 0.3506  loss_rpn_loc: 0.2028  time: 0.4776  data_time: 0.0057  lr: 1.998e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:23:46 d2.utils.events]: \u001b[0m eta: 0:03:13  iter: 99  total_loss: 2.49  loss_cls: 0.5648  loss_box_reg: 0.7067  loss_mask: 0.6345  loss_rpn_cls: 0.3467  loss_rpn_loc: 0.2139  time: 0.4821  data_time: 0.0057  lr: 2.4975e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:23:56 d2.utils.events]: \u001b[0m eta: 0:03:06  iter: 119  total_loss: 2.322  loss_cls: 0.5451  loss_box_reg: 0.702  loss_mask: 0.6021  loss_rpn_cls: 0.2873  loss_rpn_loc: 0.1841  time: 0.4871  data_time: 0.0050  lr: 2.997e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:24:07 d2.utils.events]: \u001b[0m eta: 0:02:58  iter: 139  total_loss: 2.326  loss_cls: 0.5364  loss_box_reg: 0.7751  loss_mask: 0.569  loss_rpn_cls: 0.2318  loss_rpn_loc: 0.1848  time: 0.4941  data_time: 0.0058  lr: 3.4965e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:24:18 d2.utils.events]: \u001b[0m eta: 0:02:50  iter: 159  total_loss: 2.321  loss_cls: 0.5309  loss_box_reg: 0.7643  loss_mask: 0.5415  loss_rpn_cls: 0.2916  loss_rpn_loc: 0.196  time: 0.5001  data_time: 0.0052  lr: 3.996e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:24:29 d2.utils.events]: \u001b[0m eta: 0:02:43  iter: 179  total_loss: 2.227  loss_cls: 0.5221  loss_box_reg: 0.76  loss_mask: 0.5152  loss_rpn_cls: 0.2379  loss_rpn_loc: 0.1966  time: 0.5044  data_time: 0.0047  lr: 4.4955e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:24:40 d2.utils.events]: \u001b[0m eta: 0:02:34  iter: 199  total_loss: 2.222  loss_cls: 0.5186  loss_box_reg: 0.7908  loss_mask: 0.4751  loss_rpn_cls: 0.2374  loss_rpn_loc: 0.2052  time: 0.5089  data_time: 0.0048  lr: 4.995e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:24:51 d2.utils.events]: \u001b[0m eta: 0:02:25  iter: 219  total_loss: 2.147  loss_cls: 0.5096  loss_box_reg: 0.7919  loss_mask: 0.4595  loss_rpn_cls: 0.2044  loss_rpn_loc: 0.1921  time: 0.5125  data_time: 0.0052  lr: 5.4945e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:25:02 d2.utils.events]: \u001b[0m eta: 0:02:15  iter: 239  total_loss: 2.105  loss_cls: 0.4962  loss_box_reg: 0.798  loss_mask: 0.4369  loss_rpn_cls: 0.2108  loss_rpn_loc: 0.1656  time: 0.5147  data_time: 0.0049  lr: 5.994e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:25:12 d2.utils.events]: \u001b[0m eta: 0:02:05  iter: 259  total_loss: 2.094  loss_cls: 0.4974  loss_box_reg: 0.7927  loss_mask: 0.4244  loss_rpn_cls: 0.1999  loss_rpn_loc: 0.1744  time: 0.5166  data_time: 0.0049  lr: 6.4935e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:25:23 d2.utils.events]: \u001b[0m eta: 0:01:55  iter: 279  total_loss: 2.071  loss_cls: 0.485  loss_box_reg: 0.8034  loss_mask: 0.4105  loss_rpn_cls: 0.1834  loss_rpn_loc: 0.1782  time: 0.5182  data_time: 0.0052  lr: 6.993e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:25:34 d2.utils.events]: \u001b[0m eta: 0:01:45  iter: 299  total_loss: 2.011  loss_cls: 0.4876  loss_box_reg: 0.7901  loss_mask: 0.4047  loss_rpn_cls: 0.1655  loss_rpn_loc: 0.1874  time: 0.5206  data_time: 0.0051  lr: 7.4925e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:25:45 d2.utils.events]: \u001b[0m eta: 0:01:35  iter: 319  total_loss: 2.087  loss_cls: 0.4913  loss_box_reg: 0.7898  loss_mask: 0.4018  loss_rpn_cls: 0.1994  loss_rpn_loc: 0.1669  time: 0.5221  data_time: 0.0051  lr: 7.992e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:25:56 d2.utils.events]: \u001b[0m eta: 0:01:24  iter: 339  total_loss: 1.974  loss_cls: 0.4719  loss_box_reg: 0.7908  loss_mask: 0.383  loss_rpn_cls: 0.1622  loss_rpn_loc: 0.1762  time: 0.5230  data_time: 0.0056  lr: 8.4915e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:26:07 d2.utils.events]: \u001b[0m eta: 0:01:14  iter: 359  total_loss: 1.979  loss_cls: 0.4772  loss_box_reg: 0.7858  loss_mask: 0.374  loss_rpn_cls: 0.1517  loss_rpn_loc: 0.1684  time: 0.5245  data_time: 0.0051  lr: 8.991e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:26:18 d2.utils.events]: \u001b[0m eta: 0:01:03  iter: 379  total_loss: 1.96  loss_cls: 0.463  loss_box_reg: 0.7768  loss_mask: 0.3764  loss_rpn_cls: 0.1644  loss_rpn_loc: 0.1504  time: 0.5254  data_time: 0.0051  lr: 9.4905e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:26:29 d2.utils.events]: \u001b[0m eta: 0:00:53  iter: 399  total_loss: 1.844  loss_cls: 0.4487  loss_box_reg: 0.7455  loss_mask: 0.3524  loss_rpn_cls: 0.136  loss_rpn_loc: 0.1606  time: 0.5267  data_time: 0.0050  lr: 9.99e-05  max_mem: 2564M\n","\u001b[32m[01/01 08:26:40 d2.utils.events]: \u001b[0m eta: 0:00:42  iter: 419  total_loss: 1.858  loss_cls: 0.4524  loss_box_reg: 0.7431  loss_mask: 0.3756  loss_rpn_cls: 0.1119  loss_rpn_loc: 0.1597  time: 0.5273  data_time: 0.0051  lr: 0.0001049  max_mem: 2564M\n","\u001b[32m[01/01 08:26:51 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 439  total_loss: 1.836  loss_cls: 0.4402  loss_box_reg: 0.7301  loss_mask: 0.359  loss_rpn_cls: 0.1328  loss_rpn_loc: 0.1666  time: 0.5284  data_time: 0.0049  lr: 0.00010989  max_mem: 2564M\n","\u001b[32m[01/01 08:27:01 d2.utils.events]: \u001b[0m eta: 0:00:21  iter: 459  total_loss: 1.815  loss_cls: 0.4362  loss_box_reg: 0.6996  loss_mask: 0.3621  loss_rpn_cls: 0.1454  loss_rpn_loc: 0.1786  time: 0.5282  data_time: 0.0050  lr: 0.00011489  max_mem: 2564M\n","\u001b[32m[01/01 08:27:12 d2.utils.events]: \u001b[0m eta: 0:00:10  iter: 479  total_loss: 1.764  loss_cls: 0.4054  loss_box_reg: 0.6657  loss_mask: 0.3516  loss_rpn_cls: 0.134  loss_rpn_loc: 0.1663  time: 0.5286  data_time: 0.0051  lr: 0.00011988  max_mem: 2564M\n","\u001b[32m[01/01 08:27:24 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 1.73  loss_cls: 0.4219  loss_box_reg: 0.6632  loss_mask: 0.3466  loss_rpn_cls: 0.1325  loss_rpn_loc: 0.1714  time: 0.5282  data_time: 0.0052  lr: 0.00012488  max_mem: 2564M\n","\u001b[32m[01/01 08:27:24 d2.engine.hooks]: \u001b[0mOverall training speed: 498 iterations in 0:04:23 (0.5283 s / it)\n","\u001b[32m[01/01 08:27:24 d2.engine.hooks]: \u001b[0mTotal training time: 0:04:25 (0:00:02 on hooks)\n","\u001b[32m[01/01 08:27:25 d2.data.datasets.coco]: \u001b[0mLoaded 338 images in COCO format from /content/drive/MyDrive/wheat-coco/archive (1)/coco/annotations/instances_val2017.json\n","\u001b[32m[01/01 08:27:25 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|   wheat    | 15190        |\n","|            |              |\u001b[0m\n","\u001b[32m[01/01 08:27:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[01/01 08:27:25 d2.data.common]: \u001b[0mSerializing 338 elements to byte tensors and concatenating them all ...\n","\u001b[32m[01/01 08:27:25 d2.data.common]: \u001b[0mSerialized dataset takes 1.67 MiB\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/01 08:27:25 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n","\u001b[32m[01/01 08:27:26 d2.data.datasets.coco]: \u001b[0mLoaded 338 images in COCO format from /content/drive/MyDrive/wheat-coco/archive (1)/coco/annotations/instances_val2017.json\n","\u001b[32m[01/01 08:27:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[01/01 08:27:26 d2.data.common]: \u001b[0mSerializing 338 elements to byte tensors and concatenating them all ...\n","\u001b[32m[01/01 08:27:26 d2.data.common]: \u001b[0mSerialized dataset takes 1.67 MiB\n","\u001b[32m[01/01 08:27:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 338 images\n","\u001b[32m[01/01 08:27:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/338. 0.1300 s / img. ETA=0:02:19\n","\u001b[32m[01/01 08:27:37 d2.evaluation.evaluator]: \u001b[0mInference done 23/338. 0.1312 s / img. ETA=0:02:17\n","\u001b[32m[01/01 08:27:42 d2.evaluation.evaluator]: \u001b[0mInference done 35/338. 0.1306 s / img. ETA=0:02:11\n","\u001b[32m[01/01 08:27:47 d2.evaluation.evaluator]: \u001b[0mInference done 47/338. 0.1309 s / img. ETA=0:02:06\n","\u001b[32m[01/01 08:27:52 d2.evaluation.evaluator]: \u001b[0mInference done 59/338. 0.1304 s / img. ETA=0:02:00\n","\u001b[32m[01/01 08:27:57 d2.evaluation.evaluator]: \u001b[0mInference done 71/338. 0.1307 s / img. ETA=0:01:55\n","\u001b[32m[01/01 08:28:03 d2.evaluation.evaluator]: \u001b[0mInference done 83/338. 0.1311 s / img. ETA=0:01:50\n","\u001b[32m[01/01 08:28:08 d2.evaluation.evaluator]: \u001b[0mInference done 95/338. 0.1313 s / img. ETA=0:01:45\n","\u001b[32m[01/01 08:28:13 d2.evaluation.evaluator]: \u001b[0mInference done 107/338. 0.1316 s / img. ETA=0:01:40\n","\u001b[32m[01/01 08:28:18 d2.evaluation.evaluator]: \u001b[0mInference done 119/338. 0.1320 s / img. ETA=0:01:35\n","\u001b[32m[01/01 08:28:24 d2.evaluation.evaluator]: \u001b[0mInference done 131/338. 0.1323 s / img. ETA=0:01:30\n","\u001b[32m[01/01 08:28:29 d2.evaluation.evaluator]: \u001b[0mInference done 143/338. 0.1323 s / img. ETA=0:01:25\n","\u001b[32m[01/01 08:28:34 d2.evaluation.evaluator]: \u001b[0mInference done 155/338. 0.1324 s / img. ETA=0:01:19\n","\u001b[32m[01/01 08:28:40 d2.evaluation.evaluator]: \u001b[0mInference done 167/338. 0.1325 s / img. ETA=0:01:14\n","\u001b[32m[01/01 08:28:45 d2.evaluation.evaluator]: \u001b[0mInference done 179/338. 0.1324 s / img. ETA=0:01:09\n","\u001b[32m[01/01 08:28:50 d2.evaluation.evaluator]: \u001b[0mInference done 191/338. 0.1324 s / img. ETA=0:01:04\n","\u001b[32m[01/01 08:28:55 d2.evaluation.evaluator]: \u001b[0mInference done 203/338. 0.1323 s / img. ETA=0:00:59\n","\u001b[32m[01/01 08:29:01 d2.evaluation.evaluator]: \u001b[0mInference done 215/338. 0.1322 s / img. ETA=0:00:53\n","\u001b[32m[01/01 08:29:06 d2.evaluation.evaluator]: \u001b[0mInference done 227/338. 0.1322 s / img. ETA=0:00:48\n","\u001b[32m[01/01 08:29:11 d2.evaluation.evaluator]: \u001b[0mInference done 239/338. 0.1322 s / img. ETA=0:00:43\n","\u001b[32m[01/01 08:29:16 d2.evaluation.evaluator]: \u001b[0mInference done 251/338. 0.1322 s / img. ETA=0:00:38\n","\u001b[32m[01/01 08:29:22 d2.evaluation.evaluator]: \u001b[0mInference done 263/338. 0.1322 s / img. ETA=0:00:32\n","\u001b[32m[01/01 08:29:27 d2.evaluation.evaluator]: \u001b[0mInference done 275/338. 0.1322 s / img. ETA=0:00:27\n","\u001b[32m[01/01 08:29:32 d2.evaluation.evaluator]: \u001b[0mInference done 287/338. 0.1323 s / img. ETA=0:00:22\n","\u001b[32m[01/01 08:29:37 d2.evaluation.evaluator]: \u001b[0mInference done 299/338. 0.1324 s / img. ETA=0:00:17\n","\u001b[32m[01/01 08:29:43 d2.evaluation.evaluator]: \u001b[0mInference done 311/338. 0.1324 s / img. ETA=0:00:11\n","\u001b[32m[01/01 08:29:48 d2.evaluation.evaluator]: \u001b[0mInference done 323/338. 0.1324 s / img. ETA=0:00:06\n","\u001b[32m[01/01 08:29:53 d2.evaluation.evaluator]: \u001b[0mInference done 335/338. 0.1324 s / img. ETA=0:00:01\n","\u001b[32m[01/01 08:29:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:25.547862 (0.437081 s / img per device, on 1 devices)\n","\u001b[32m[01/01 08:29:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:44 (0.132386 s / img per device, on 1 devices)\n","\u001b[32m[01/01 08:29:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n","\u001b[32m[01/01 08:29:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n","\u001b[32m[01/01 08:29:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n","Loading and preparing results...\n","DONE (t=0.02s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","COCOeval_opt.evaluate() finished in 0.52 seconds.\n","Accumulating evaluation results...\n","COCOeval_opt.accumulate() finished in 0.07 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.747\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.209\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.320\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.111\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.425\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.426\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.437\n","\u001b[32m[01/01 08:29:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n","|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n","|:------:|:------:|:------:|:-----:|:------:|:------:|\n","| 31.709 | 74.726 | 20.884 | 1.537 | 32.028 | 32.986 |\n","Loading and preparing results...\n","DONE (t=0.39s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *segm*\n","COCOeval_opt.evaluate() finished in 1.20 seconds.\n","Accumulating evaluation results...\n","COCOeval_opt.accumulate() finished in 0.06 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.281\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.737\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.290\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.276\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.010\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.097\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.404\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.357\n","\u001b[32m[01/01 08:29:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n","|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n","|:------:|:------:|:------:|:-----:|:------:|:------:|\n","| 28.113 | 73.688 | 11.769 | 1.667 | 28.953 | 27.556 |\n","OrderedDict([('bbox', {'AP': 31.708852354775086, 'AP50': 74.72590251393918, 'AP75': 20.884057029426216, 'APs': 1.5373625246988643, 'APm': 32.02847270165599, 'APl': 32.98615238590508}), ('segm', {'AP': 28.113364461949985, 'AP50': 73.68801369951468, 'AP75': 11.769453843189135, 'APs': 1.6668725560455542, 'APm': 28.952565504177862, 'APl': 27.55620659964783})])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7mN1NJPyUnXl","executionInfo":{"status":"ok","timestamp":1609489854324,"user_tz":-330,"elapsed":886,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}}},"source":["def customDataset_test(test_annotations):\n","    test=[]\n","    test_dict={}\n","    for (k, v) in test_annotations[\"images\"].items():\n","        test_dict[\"file_name\"]=v[\"file_name\"]\n","        test_dict[\"height\"]=v[\"height\"]\n","        test_dict[\"width\"]=v[\"width\"]\n","        test_dict[\"image_id\"]=v[\"id\"]\n","        test_dict[\"annotations\"][\"bbox\"]=test_annotations[\"annotations\"][k][\"bbox\"].values()\n","        test_dict[\"annotations\"][\"bbox_mode\"]=1\n","        test_dict[\"annotations\"][\"category_id\"]=test_annotations[\"annotations\"][k][\"category_id\"]\n","        test_dict[\"annotations\"][\"segmentation\"]=test_annotations[\"annotations\"][k][\"segmentation\"].values()\n","        test.append(test_dict)\n","        test_dict={}\n","    return test"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPolcLzDcxDo","executionInfo":{"status":"ok","timestamp":1609489858297,"user_tz":-330,"elapsed":901,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}}},"source":["def _json_decode(json_bytes, encoding):\n","    tiow = io.TextIOWrapper(\n","        io.BytesIO(json_bytes), encoding=encoding, newline=\"\"\n","    )\n","    obj = json.load(tiow)\n","    tiow.close()\n","    return obj"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"HfX8JWW4Uqbo","executionInfo":{"status":"ok","timestamp":1609489865498,"user_tz":-330,"elapsed":904,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}}},"source":["test_dataset=[]\n","input_test=b\"\"\n","with open(dir+r\"annotations/instances_val2017.json\", \"rb\") as f_val:\n","    #print(f.readable())\n","    input_test = f_val.read()\n","    test_dataset = _json_decode(input_test, \"utf-8-sig\")\n","    #print(test_annotations[\"images\"])\n","    #test_dataset=customDataset_test(test_annotations)\n","    f_val.close()\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzfIDWkRWOoD","executionInfo":{"status":"ok","timestamp":1609489870384,"user_tz":-330,"elapsed":1874,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}}},"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n","predictor = DefaultPredictor(cfg)\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhtnZFABWRze","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZhTlKtQfVAFA2R0sVQRxWvDHVKjvEsf2"},"executionInfo":{"status":"ok","timestamp":1609489934410,"user_tz":-330,"elapsed":7117,"user":{"displayName":"Premkumar Nandakumar","photoUrl":"","userId":"06072416369964963255"}},"outputId":"81620b88-c96c-4964-a1c0-46d95a5f16da"},"source":["from google.colab.patches import cv2_imshow\n","for dicts in random.sample(test_dataset[\"images\"], 5):    \n","    img = cv2.imread(dir+r\"images/val2017/\"+dicts[\"file_name\"])\n","    outputs = predictor(img)  \n","    v = Visualizer(img[:, :, ::-1],\n","                   metadata=MetadataCatalog.get(\"customDataset_test\"), \n","                   scale=0.5, \n","                   instance_mode=ColorMode.IMAGE_BW   \n","    )\n","    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    print(dicts[\"file_name\"])\n","    cv2_imshow(out.get_image()[:, :, ::-1])\n"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}